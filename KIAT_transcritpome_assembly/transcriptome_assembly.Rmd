---
title: "Transcritpome_assembly"
author: "Ruijuan Li"
date: "6/26/2017"
output: 
  html_document: 
    keep_md: yes
---

#### download data & trimming 
```{r}
# 1) download raw fastq data using wget 

# 2) check data quality using fastqc 

# 3) Trimm off low quality reads & adapter contamination 
# trimmomatic PE Sample_${sample}/${sample}_1.fq.gz Sample_${sample}/${sample}_2.fq.gz Sample_${sample}/${sample}_paired_1.fq.gz Sample_${sample}/${sample}_unpaired_1.fq.gz Sample_${sample}/${sample}_paired_2.fq.gz Sample_${sample}/${sample}_unpaired_2.fq.gz ILLUMINACLIP:Bradseq_adapter.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36

# 4) map high quality reads to reference genome using STAR
# 4.1) add "gene_id" colomn to gff3 file 
# run with modified gff3 file 
# STAR --runMode genomeGenerate --genomeDir star_genome/ --genomeFastaFiles Brassica_napus_v4.1.chromosomes.fa --sjdbGTFfile Brassica_napus.annotation_v5_modified_modified.gff3 --runThreadN 6 --sjdbGTFtagExonParentTranscript Parent --sjdbGTFfeatureExon CDS (screen -r 10070.ttys003.coloma) 

############# reference based ##################### 
# 4.2)  mapping 
# "STAR --genomeDir /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/Reference/B.napus/star_genome --readFilesIn /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/2016_summer/raw_data/Ae_Hu_2_1.fq /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/2016_summer/raw_data/Ae_Hu_2_2.fq --outSAMtype BAM SortedByCoordinate --sjdbGTFfile /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/Reference/B.napus/Brassica_napus.annotation_v5_modified_modified.gff3 --quantMode TranscriptomeSAM GeneCounts --twopassMode Basic â€“alignIntronMax 15000 --outFilterIntronMotifs RemoveNoncanonical --runThreadN 6 --sjdbGTFtagExonParentTranscript Parent --sjdbGTFfeatureExon CDS --outReadsUnmapped Fastx" 

# 5) assembly 
# 5.1) use IGV to decide library type 
# after visualize genome, bam file, and gff file, we decide our library type is fr-secondstrand 
# 5.2) assemble each library seperately using cufflinks 
# transcriptome assembly using original gff3 file
# cufflinks  -o /share/malooflab/Ruijuan/parent_assembly/${sample}/cufflink_output/ \
# 	   --junc-alpha 0.001 \
# 	   --label CUFF \
# 	   --max-bundle-length 3500000 \
# 	   --max-intron-length 300000 \
# 	   --max-mle-iterations 5000 \
# 	   --min-frags-per-transfrag 10 \
# 	   --min-intron-length 50 \
# 	   --min-isoform-fraction 0.1 \
# 	   --no-update-check \
# 	   --num-importance-samples 1000 \
# 	   --num-threads 20 \
# 	   --overhang-tolerance 8 \
# 	   --pre-mrna-fraction 0.15 \
# 	   --small-anchor-fraction 0.09 \
# 	   --trim-3-avgcov-thresh 10 \
# 	   --trim-3-dropoff-frac 0.1 \
# 	   --library-type fr-secondstrand \
# 	   -g /share/malooflab/Ruijuan/reference/Brassica_napus.annotation_v5.gff3 \
# 	   -b /home/ruijuanli/2017_winter/assembly/Brassica_napus_v4.1.chromosomes.fa \
# 	    /share/malooflab/Ruijuan/parent_assembly/${sample}/Aligned.sortedByCoord.out.bam

# 6) merge seperate assembly into one using cuffmerge 
# cuffmerge 
# https://github.com/leejimmy93/KIAT_whitney/blob/master/parent_RNA_transcriptome/run_cuffmerge_Ae.sh 

# https://github.com/leejimmy93/KIAT_whitney/blob/master/parent_RNA_transcriptome/run_cuffmerge_Ol.sh

# 7) compare to current annotation for the reference genome  
# cuffcompare 

# https://github.com/leejimmy93/KIAT_whitney/blob/master/parent_RNA_transcriptome/run_cuffcompare_Ae.sh
# https://github.com/leejimmy93/KIAT_whitney/blob/master/parent_RNA_transcriptome/run_cuffcompare_Ol.sh 

# 8) remove redundant isoforms from novel transcripts using CAP3 
# https://github.com/leejimmy93/KIAT_whitney/blob/master/parent_RNA_transcriptome/run_cap3.sh

# 9) predict ORF from non-redundant transcript dataset using transdecoder 
# TransDecoder.LongOrfs -t Ae.u.fa.cap.contigs_singlets 
# TransDecoder.LongOrfs -t Ol.u.fa.cap.contigs_singlets 

# blastp against nr protein database to get an idea of what genes they are 
# blastp -query longest_orfs.pep  -db /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/Reference/NCBI/nr/nr  -max_target_seqs 1 -outfmt 6 -evalue 1e-6 -num_threads 6 -out ${i}_nr.out  

# 10) multiple ORF were produced for each transcript, the number of transcripts with ORF identified are: 
# Da-Ae (2311) & Da-Ol-1 (2427) : cat longest_orfs.pep | grep ">" | sed 's/:/\t/g' | awk '{print $2}' | sort | uniq | wc -l 

# 11) the number of novel genes which can be annotated 
# cat longest_orfs.pep | grep ">" | sed 's/:/\t/g' | awk '{print $2}' | sort | uniq | wc -l
# for Da-Ae 2311; for Da-Ol-1 2427 

# extract sequences that need go for annotation 
# cat longest_orfs.pep | grep ">" | sed 's/:/\t/g' | awk '{print $2}' | sort | uniq > for_annotation

# perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/assembly_parent/transdecoder/after_cap3/Ol.u.fa.cap.contigs_singlets.transdecoder_dir/for_annotation_ID Ol.u.fa.cap.contigs_singlets > Ol_ref_for_annotation.fa  

# /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/assembly_parent/reference_based_for_annotation/Ae_ref_for_annotation_final.fa  & 
# /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/assembly_parent/reference_based_for_annotation/Ol_ref_for_annotation.fa 


```


# From Calen 
```{r}
#### de-novo based 
# 1) trim each lib 

# 2) combine trimmed data for Da-Ae & Da-Ol-1 

# 3) run trinity on Da-Ae & Da-Ol-1: stats, N50, how many transcripts? ... length distribution?  
# Da-Ae: genes: 177731; transcripts: 476452; N50_all contigs: 761; 
# Da-Ol-1:  genes: 187252; transcripts: 497089; N50_all contigs: 775;

# 4) get read count for transcripts of Da-Ae & Da-Ol-1, remove transcripts with reads number less than 1: stats? how many transcripts left? length distribution  
# Da-Ae: genes: 50120; transcripts: 102127
# Da-Ol-1: genes: 47464; transcripts: 93678

# 5) predict ORF from transcript dataset using transdecoder: stats, how many transcripts? 
# 75662 for Da-Ae; 69906 for Da-Ol-1 

# 6) blast against B.rapa CDS to extract novel transcript not present in the existing B.rapa genome annotation 
# Da-Ae: 73346 with B.rapa CDS hit (e-value < 1e-6); 67894 above 95% percent identity, so (75662-67894) 7,768 novel genes 
# Da-Ol-1: 67883 with B.rapa CDS hit (e-value < 1e-6); 62905 above 95% percent identity, so (69906-62905) 7,001 novel genes 

# 7) BWA-MEM 7768 Ae genes and 7001 Ol genes to B.napus genome to see how many of them hit B.napus genome 

## unmapped: 3rd column is "*"
# cat Ae_bwa_mem.sam | awk 'NF > 8{print $3}' | sort | uniq -c # 139 unmapped 
# cat Ol_bwa_mem.sam | awk 'NF > 8{print $3}' | sort | uniq -c # 90 unmapped 

# uniquely mapped: absence of "XA:Z" means unqiue mapping 
# cat Ae_bwa_mem.sam | awk 'NF > 8' | grep -v "XA:Z" | awk '{print $1}' | sort | uniq | wc -l 4728
# cat Ol_bwa_mem.sam | awk 'NF > 8' | grep -v "XA:Z" | awk '{print $1}' | sort | uniq | wc -l 4118

# Da-Ae: 4728 genes are uniquely mapped to B.napus genome; 
# Da-Ol-1: 4118 genes are uniquely mapped to B.napus genome; 

# 8) blastx against Uniref protein database (on whitney)
# Da-Ae: 4076 blastx against 3690 Uniref protein (for unknown reason, not every Uniref protein has annotation here using batch entrez, but the ones that are proved to be plant genes are 2873, 10 non-plant genes)
# Da-Ol: 3775 blastx against 3444 Uniref protein (2628 plant genes, and 15 non-plant genes)

# 9) tag uniquely mapped & uniquely mapped genes, blastx unmapped genes to NCBI nr database to see whether they hit plant proteins 
# 39 genes out 139 genes of Da-Ae are found to have blast hit in plant 
# 24 (-1) genes out of 90 genes of Da-Ol-1 are found to have blast hit in plant 

# 10) remove chimera RNA: chimera RNAs are composed of exons from two or more different genes and the potential to encode novel proteins. While natual chimeric transcripts exist in some cancer tissues but are very rare, and most of the cases they are resulted from mis-assembly in transcriptome assembly. So they need to be removed...    

# 11) combine ref based & de-novo assembly 
# Da-Ae 6065+2320
# Da-Ol-1 5548+2427

# 12) after CAP3
# Da-Ae: 6622
# Da-Ol-1: 6430 

# 13) dammit for annotation 

# PASA pipeline  
#1) seqclean to identify transcript with polyAtail 
# ... 
```

# filtering based on ID
```{r}
library(tidyverse)
Ae_unmapped.cds_ID <- read.table(file = "~/assembly_parent/De_novo/filtering/Ae_unmapped.cds_ID")

Ae_unmapped.cds_ID %>% head()
Ae_unmapped.blastx.trinity <- read.csv("~/assembly_parent/De_novo/filtering/Ae_unmapped.blastx.trinity2")


```

#### used for chimera
* get size info for all transcript, used in chimera remove in Uchime 

# structure annotation (possible tool: maker2)
=======
* De-novo Assembly
```{r}
# 1) In directory Users/calenbadger/De_novo_Assembly
# Run ./trimming.sh in a screen # (Trimmomatic + Illuminaclip)

# 2) Unzipping and Concatenating
# In directory Users/calenbadger/De_novo_Assembly/Trimmed_Fastqs
# zcat 05*1.p* 5_1.p* 6_1.p* 8_1.p* Ae*1.p* | gzip -c > Ae_Trimmed_1.paired.fq.gz
# zcat 05*2.p* 5_2.p* 6_2.p* 8_2.p* Ae*2.p* | gzip -c > Ae_Trimmed_2.paired.fq.gz
# zcat 1_1.p* 2_1.p* 3_1.p* 4_1.p* All1*1.p* | gzip -c > Ol_Trimmed_1.paired.fq.gz 
# zcat 1_2.p* 2_2.p* 3_2.p* 4_2.p* All1*2.p* | gzip -c > Ol_Trimmed_2.paired.fq.gz

# 3) Run Abundance Script for Ae and Ol
# In /calenbadger/De_novo_Assembly
# Option 1: kallisto (Ae)
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/align_and_estimate_abundance.pl \
# --transcripts ./Da_Ae_Trinity.fasta.gz \
# --seqType fq \
# --left ./Trimmed_Fastqs/Ae_Trimmed_1.paired.fq.gz \
# --right ./Trimmed_Fastqs/Ae_Trimmed_2.paired.fq.gz \
# --est_method kallisto (RSEM|eXpress|kallisto|salmon) \
# --output_dir ~/De_novo_Assembly/Ae_alignment_output \
# --SS_lib_type FR (RF|FR) \
# --thread_count 4 \
# --trinity_mode (generates gene_trans_map and uses it)(gene_trans_map contains components, genes, and isoforms) \
# --prep_reference
# --output_prefix kallisto (default --est_method setting)

# Kallisto (Ol)
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/align_and_estimate_abundance.pl \
# --transcripts ./Da_Ol_1_Trinity.fasta.gz \
# --seqType fq \
# --left ./Trimmed_Fastqs/Ol_Trimmed_1.paired.fq.gz \
# --right ./Trimmed_Fastqs/Ol_Trimmed_2.paired.fq.gz \
# --est_method kallisto (RSEM|eXpress|kallisto|salmon) \
# --output_dir ~/De_novo_Assembly/Ol_alignment_output \
# --SS_lib_type FR (RF|FR) \
# --thread_count 4 \
# --trinity_mode (generates gene_trans_map and uses it)(gene_trans_map contains components, genes, and isoforms) \
# --prep_reference
# --output_prefix kallisto (default --est_method setting)

# Option 2: RSEM
# perl /usr/local/bin/Trinity/util/align_and_estimate_abundance.pl \
# --transcripts <string> \
# --seqType fq \
# --left <string> \
# --right <string> \
# --est_method RSEM (RSEM|eXpress|kallisto|salmon) \
# --output_dir ~/De_novo_Assembly/alignment_output \
# --aln_method <string> (bowtie|bowtie2|path_to_bam_file)
# --SS_lib_type FR (RF|FR) \
# --thread_count 4 \
# --trinity_mode (generates gene_trans_map and uses it) \
# --output_prefix RSEM (default --est_method setting)

# 4) Build Gene Expression Matrices
# In De_novo_Assembly
# For Both Samples Together
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/abundance_estimates_to_matrix.pl \
# --est_method kallisto \
# --out_prefix Ae_Ol_counts \
# --name_sample_by_basedir \
# Ae_alignment_output/abundance.tsv \
# Ol_alignment_output/abundance.tsv

# For Ae Only
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/abundance_estimates_to_matrix.pl --est_method kallisto --out_prefix Ae_counts ~/De_novo_Assembly/Ae_alignment_output/abundance.tsv

# For Ol Only
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/abundance_estimates_to_matrix.pl --est_method kallisto --out_prefix Ol_counts ~/De_novo_Assembly/Ol_alignment_output/abundance.tsv

# 5) Statistics Using contig_ExN50_statistic.pl
# No Command for Combined (Requires Trinity.fasta equivalent, combining both Ae/Ol fasta files)
# In ~/De_novo_Assembly/
# Ae Statistics
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/misc/contig_ExN50_statistic.pl Ae_counts.TPM.not_cross_norm Da_Ae_Trinity.fasta | tee Ae_ExN50.stats
# Ol Statistics
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/misc/contig_ExN50_statistic.pl Ol_counts.TPM.not_cross_norm Da_Ol_1_Trinity.fasta | tee Ol_ExN50.stats

# 6) Filtering Based on Expression TPM
# Moving data into R
# Ae_abundance <- read.delim("../De_novo_Assembly/Ae_alignment_output/abundance.tsv")
# Ol_abundance <- read.delim("../De_novo_Assembly/Ol_alignment_output/abundance.tsv")

# abundance.tsv files subsetted to only include tpm expression values over 1
# Ae_exptemp <- Ae_abundance[Ae_abundance$tpm >= 1,]
# Ae_exp <- Ae_exptemp[Ae_exptemp$length <= 8000,]
# Ol_exptemp <- Ol_abundance[Ol_abundance$tpm >= 1,]
# Ol_exp <- Ol_exptemp[Ol_exptemp$length <= 8000,]

# Extracting a list of target IDs
# Ae_exp_ids <- data.frame(Ae_exp$target_id)
# Ol_exp_ids <- data.frame(Ol_exp$target_id)

# Moving ID lists to a file
# write.table(Ae_exp_ids,file="../De_novo_Assembly/Ae_exp_ids",quote=FALSE,row.names=FALSE)
# write.table(Ol_exp_ids,file="../De_novo_Assembly/Ol_exp_ids",quote=FALSE,row.names=FALSE)

# Running list of expression subset against our original fasta files to filter the contigs
# perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' Ae_exp_ids Da_Ae_Trinity.fasta > Da_Trinity_Exp.fasta
# perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' Ol_exp_ids Da_Ol_1_Trinity.fasta > Da_Ol_Trinity_Exp.fasta

# 7) ORF Extraction Pipeline
# (Complete and Partial Long ORFs, 200aa+)
# PATH=$PATH:~/bin/TransDecoder-3.0.1/
# TransDecoder.LongOrfs -t Da_Ae_Trinity_Exp.fasta
# TransDecoder.LongOrfs -t Da_Ol_Trinity_Exp.fasta

# 8) Blastn (Nucleotide Identity)
# In De_novo_Assembly Directory

# Brassica Rapa CDS File
# wget http://www.genoscope.cns.fr/brassicanapus/data/Brassica_napus.annotation_v5.cds.fa.gz
# gzip -c Brassica_napus.annotation_v5.cds.fa.gz > Brassica_napus.annotation_v5.cds.fa

# Generating Database from CDS File
# makeblastdb -in ./Brassica_napus.annotation_v5.cds.fa -dbtype â€˜nuclâ€™ -input_type 'fasta' -out NapusNuc.db -title NapusNuc

# Running Blastn
# For Ae
# blastn -query ./Ae_Transdecoder/longest_orfs.cds -db /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/De_novo_Assembly/NapusNuc.db -max_target_seqs 1 -outfmt 6 -evalue 1e-6 -num_threads 5 -out Da_Ae_NapusNuc.out
# For Ol
# blastn -query ./Ol_Transdecoder/longest_orfs.cds -db /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/De_novo_Assembly/NapusNuc.db -max_target_seqs 1 -outfmt 6 -evalue 1e-6 -num_threads 5 -out Da_Ol_NapusNuc.out

# Filtering Percent ID >= 95%
# Ae_Blast <- read.delim("../De_novo_Assembly/Da_Ae_NapusNuc.out",header=FALSE)
# Ol_Blast <- read.delim("../De_novo_Assembly/Da_Ol_NapusNuc.out",header=FALSE)
# colnames(Ae_Blast) <- c("qseqid","sseqid","pident","length","mismatch","gapopen","qstart","qend","sstart","send","evalue","bitscore")
# colnames(Ol_Blast) <- c("qseqid","sseqid","pident","length","mismatch","gapopen","qstart","qend","sstart","send","evalue","bitscore")
# Ae_Blast_PI <- Ae_Blast[Ae_Blast$pident>=95,]
# Ol_Blast_PI <- Ol_Blast[Ol_Blast$pident>=95,]
# rownames(Ae_Blast_PI) <- NULL
# rownames(Ol_Blast_PI) <- NULL
# write.table(Ae_Blast_PI,file="../De_novo_Assembly/Ae_Blast_PI95",quote=FALSE,row.names=FALSE)
# write.table(Ol_Blast_PI,file="../De_novo_Assembly/Ol_Blast_PI95",quote=FALSE,row.names=FALSE)

# Removing IDs with Over 95% Identity
# IDs for the Files were Extracted via Command Line
# Ae_orfs_ID <- read.delim("../De_novo_Assembly/Da_Ae_Trinity_Exp.fasta.transdecoder_dir/longest_orfs.cds_ID",header=FALSE,col.names="ID")
# Ol_orfs_ID <- read.delim("../De_novo_Assembly/Da_Ol_Trinity_Exp.fasta.transdecoder_dir/longest_orfs.cds_ID",header=FALSE,col.names="ID")
# Ae_95_ID <- read.delim("../De_novo_Assembly/Ae_Blast_PI95_ID",header=FALSE,col.names="ID")
# Ol_95_ID <- read.delim("../De_novo_Assembly/Ol_Blast_PI95_ID",header=FALSE,col.names="ID")

# Package Installs
# install.packages("magrittr")
# library(magrittr)
# install.packages("dplyr") #might already be present
# library(dplyr)

# Anti_Join to Remove IDs that have a match between Orf and Blast IDs
# Ae_orfs_95 <- Ae_orfs_ID %>%
  # anti_join(Ae_95_ID, by = "ID")
# Ol_orfs_95 <- Ol_orfs_ID %>%
  # anti_join(Ol_95_ID, by = "ID")

# write.table(Ae_orfs_95,file="../De_novo_Assembly/Ae_orfs_95_ID",quote=FALSE,row.names=FALSE)
# write.table(Ol_orfs_95,file="../De_novo_Assembly/Ol_orfs_95_ID",quote=FALSE,row.names=FALSE)

# Blast Identity Filtering
# perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' Ae_orfs_95_ID Ae_Transdecoder/longest_orfs.cds > ./Ae_Transdecoder/orfs_novel.cds
# perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' Ol_orfs_95_ID Ol_Transdecoder/longest_orfs.cds > ./Ol_Transdecoder/orfs_novel.cds

# 9) BWA-MEM Alignment
# bwa index Brassica_napus_v4.1.chromosomes.fa
# bwa mem Brassica_napus_v4.1.chromosomes.fa ../Ae_Transdecoder/orfs_novel.cds > Ae_bwa_mem.sam
# bwa mem Brassica_napus_v4.1.chromosomes.fa ../Ol_Transdecoder/orfs_novel.cds > Ol_bwa_mem.sam
# All Da Ae and Da Ol genes had at least 1 alignment

# 10) Cap3 Redundancy Filtering
# ./run_cap3_cds.sh
# 
# 11) Blastx on Cluster

# 12) Generating Unmapped Cds Files
# From ~/De_novo_Assembly/Bwa_Mem
# perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/De_novo_Assembly/Bwa_Mem/Ae_napus_unmapped_ID ../Ae_Transdecoder/orfs_novel.cds > ./Ae_napus_unmapped.cds
# perl -ne 'if(/^>(\S+)/){$c=$i{$1}}$c?print:chomp;$i{$_}=1 if @ARGV' /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/De_novo_Assembly/Bwa_Mem/Ol_napus_unmapped_ID ../Ol_Transdecoder/orfs_novel.cds > ./Ol_napus_unmapped.cds

# 13) Running Blastx Against NR database for unmapped cds
# blastx -query ./Ae_napus_unmapped.cds -db /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/Reference/NCBI/nr/nr -max_target_seqs 1 -outfmt 6 -evalue 1e-6 -num_threads 5 -out ./Ae_unmapped.blastx.out

# blastx -query ./Ol_napus_unmapped.cds -db /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/ruijuanli/Reference/NCBI/nr/nr -max_target_seqs 1 -outfmt 6 -evalue 1e-6 -num_threads 5 -out ./Ol_unmapped.blastx.out

# 14) Producing Final List of Genes
# Removed 1 nonplant unmapped (bwa) Ol gene
# Removed genes with no blastx hit (Ae ~100 genes, Ol ~66 genes) (Actual number of genes removed may be lower because some of the genes with blastx hits were duplicates)
# Used our list of remaining genes from the longest_orf.cds file to extract the genes and their sequence from our original expression filtered assembly

# 15) Chimera Filtering (Attempted)
# Appended TPM abundance to our novel genes
# Used Vsearch - only turned up 0% chimera results
# vsearch --uchime_denovo Ae_napus_like_gene_ID_Trinity.fa --chimeras Ae_chimera.out.fa --nonchimeras Ae_nonchimera.out.fa

```

* Reference and denovo Assembly
```{r}
# 1) 9 Genes in Reference assembly had 2 copies - Removed the extra copies

# 2) Combined denovo and Reference Based Novel Gene Assemblies
# cat Ae_napus_like_gene_ID_Trinity.fa Ae_ref_for_annotation_final.fa > Ae_combined_genes.fa
# cat Ol_napus_like_gene_ID_Trinity.fa Ol_ref_for_annotation_final.fa > Ol_combined_genes.fa

# 3) Cap3 on Combined Assemblies (Ae and Ol separate)
# In /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/cap3_denovo_reference
# ./run_cap3.sh (runs Cap3 for Ae and Ol)

# 4) Combined singlets and contigs for Ae and Ol
# cat Ae_combined_genes.fa.cap.contigs Ae_combined_genes.fa.cap.singlets > Ae_combined_genes.fa.cap.singlets_contigs
# cat Ol_combined_genes.fa.cap.contigs Ol_combined_genes.fa.cap.singlets > Ol_combined_genes.fa.cap.singlets_contigs

# 5) BUSCO Transcriptome Completeness
# In /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/assembly_parent/B.napus/busco
# for i in $files; do mkdir ${i}_dir; cd ${i}_dir; python3 /usr/local/stow/busco_python3/scripts/run_BUSCO.py -i ../${i} -o busco -l /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/assembly_parent/B.napus/busco/lineages/embryophyta_odb9 -m transcriptome -c 10; cd ..; done
# Running 5 Files in $files - Files_dir:
# Brassica_napus.annotation_v5.cds.fa_dir (Reference)
# C: 97.3%
# Ae_Ol_novel_combined_genes.fa.cap.singlets_contigs_dir (Novel Genes)
# C: 7.5%
# Brassica_napus.annotation.novel.cds.fa_dir (Reference + Novel Genes)
# C: 97.7%
# Ae_combined_genes.fa.cap.singlets_contigs_dir [Not used]
# C: 4.7%
# Ol_combined_genes.fa.cap.singlets_contigs_dir [Not used]
# C: 5.7%
# Results combined into one file
# cat *_dir/run_busco/short_summary_busco.txt | grep -v "version" | grep -v "lineage" | grep -v "reproduce" | grep -v "mode" > busco_results.txt
# Completion increased with from 97.3% to 97.7% with the addition of the novel Ae and Ol genes

```


* Structural and Functional Annotation (Dammit)
```{r}
# 1) Dammit Installation Instructions https://angus.readthedocs.io/en/2017/dammit_annotation.html#

# 2) Entering py3 instance environment (use this for dammit annotation)
# . ~/py3/bin/activate

# In /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/assembly_parent/B.napus/dammit/dammit_run
# dammit databases -h
# Options: Fungi, eukaryota, metazoa
# dammit databases --install --database-dir fungi.db --busco-group fungi
# 3) Located B.napus Protein Database
# Files for Dammit run located in ./Ae_combined_genes.fa.cap.singlets_contigs.dammit and ./Ol_combined_genes.fa.cap.singlets_contigs.dammit
# 4) Running Dammit
# dammit annotate Ae_combined_genes.fa.cap.singlets_contigs --busco-group fungi --user-databases Brassica_napus.annotation_v5.pep.fa --n_threads 8
# dammit annotate Ol_combined_genes.fa.cap.singlets_contigs --busco-group fungi --user-databases Brassica_napus.annotation_v5.pep.fa --n_threads 8

# 5) Results
# Most Useful Files: Located in /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/dammit/dammit_results
# Ae_combined.dammit.fasta
# Ae_combined.dammit.gff3
# Ae_combined.dammit.stats.json
# Ol_combined.dammit.fasta
# Ol_combined.dammit.gff3
# Ol_combined.dammit.stats.json
```

* Visualization of Intermediates
```{r}
# 1) Length Data (Reference + Before/during filtering steps)
# Outputting Length for Historical Bnapus Genome
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/misc/fasta_seq_length.pl Brassica_napus.annotation_v5.gff3.cds.fa > Brassica_napus_length.txt

# Moving Length Data into R
# Bnapus_length <- read.table("../De_novo_Assembly/Brassica_napus_length.txt",header=TRUE,col.names=c("target_id","length"))

# 2) Length Plots
# library(ggplot2)

# Sample Plots
# Bnapus_sample <- Bnapus_length[sample(nrow(Bnapus_length), 2000), ]
# Bnapus_sample_gg <- ggplot(data=Bnapus_sample,aes(x=length)) + geom_histogram()
# Ae_sample <- Ae_exp[sample(nrow(Ae_exp), 2000), ]
# Ae_sample_gg <- ggplot(data=Ae_sample,aes(x=length)) + geom_histogram()
# Ol_sample <- Ol_exp[sample(nrow(Ol_exp), 2000), ]
# Ol_sample_gg <- ggplot(data=Ol_sample,aes(x=length)) + geom_histogram()

# 3) Full Plots
# Bnapus_length_gg <- ggplot(data=Bnapus_length,aes(x=length)) + geom_histogram()
# Ae_length_gg <- ggplot(data=Ae_exp,aes(x=length)) + geom_histogram()
# Ol_length_gg <- ggplot(data=Ol_exp,aes(x=length)) + geom_histogram()
# 9) Length Distributions
# perl /usr/local/stow/trinityrnaseq-2.2.0/util/misc/fasta_seq_length.pl Brassica_napus.annotation_v5.gff3.cds.fa > Brassica_napus_length.txt
```

* Functional (GO) Annotation (tool: interproscan)
```{r}
# 1) Downloading and Extracting
# wget ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.25-64.0/interproscan-5.25-64.0-64-bit.tar.gz
# wget ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/5.25-64.0/interproscan-5.25-64.0-64-bit.tar.gz.md5
# Check to Confirm Download:
# md5sum -c interproscan-5.25-64.0-64-bit.tar.gz.md5
# tar -pxvzf interproscan-5.25-64.0-*-bit.tar.gz

# 2) Downloading Panther
# cd interproscan/data/
# wget ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/data/panther-data-11.1.tar.gz
# wget ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/data/panther-data-11.1.tar.gz.md5
# Check to Confirm Download:
# md5sum -c panther-data-11.1.tar.gz.md5
# tar -pxvzf panther-data-11.1.tar.gz

# 3) Running Interproscan
# Da Ae
# ./interproscan.sh -dp -goterms -t n -d /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/assembly_parent/B.napus/interproscan/interproscan_results/Ae_novel -i /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/assembly_parent/B.napus/dammit/dammit_results/Ae_combined.dammit.fasta
# Da Ol
# ./interproscan.sh -dp -goterms -t n -d /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/assembly_parent/B.napus/interproscan/interproscan_results/Ol_novel -i /Network/Servers/avalanche.plb.ucdavis.edu/Volumes/Mammoth/Users/calenbadger/assembly_parent/B.napus/dammit/dammit_results/Ol_combined.dammit.fasta
```


* Old MAKERv2.31.9 Notes [Not used]
>>>>>>> Calen
```{r}
# 1) Install and Configure MAKER
# wget http://yandell.topaz.genetics.utah.edu/cgi-bin/maker_license.cgi

# 2) Easy Installation
# tar -xvzf maker-2.31.9.tgz
# In ~/bin/maker/src
# perl Build.PL
# ./Build install
# ./Build installdeps (for missing PERL dependencies, requires root permission)
# ./Build installexes (for missing external programs, requires root permission)

# 3) Separate Directories (for Geno)
# In ~/bin/maker
# mkdir maker.ae
# mkdir maker.ol

# 4) Creating ctl Files
# maker -CTL 
# creates three control files (run this once in maker.ae and maker.ol)
# Edit with Nano (is there a way to automate this?)
# maker_exe.ctl contains path information for executables
# maker_bopts.ctl contains filtering stats for BLAST/Exonerate
# maker_opts.ctl contains location of input genome file

# 5) Configuring ctl Files
# Ae Genotype
# maker_exe.ctl 

# maker_bopts.ctl 

# maker_opts.ctl 
# genome=../../../../ruijuanli/Reference/B.napus/Brassica_napus_v4.1.chromosomes.fa #genome sequence (fasta file or fasta embedded in GFF3 file)
# est=../../../../ruijuanli/assembly_parent/cap3/Ae.u.fa.cap.contigs_singlets
# protein= #prot seq fasta (multiple organisms?)
# protein_gff= #aligned prot homology from external GFF3

# Ol Genotype
# maker_exe.ctl

# maker_bopts.ctl

# maker_opts.ctl
# genome= ../../../../ruijuanli/Reference/B.napus/Brassica_napus_v4.1.chromosomes.fa #genome sequence (fasta file or fasta embedded in GFF3 file)
# est=../../../../ruijuanli/assembly_parent/cap3/Ol.u.fa.cap.contigs_singlets
# protein= #prot seq fasta (multiple organisms?)
# protein_gff= #aligned prot homology from external GFF3

# Command: maker -RM_off|R
# Argument: -RM_off|R turns off all repeat masking options
# Argument: -fix_nucleotides not recognized



# [Detailed Installation Notes]

# Install and Configure Perl
# perl -v to confirm perl is installed (5.8.0+ required)

# perl -MCPAN -e shell to access CPAN shell
# installing modules (may require sudo)
# PERMISSION ISSUES HERE
# install DBI
# install DBD::SQLite
# install Proc::ProcessTable
# install threads
# install IO::All
# install IO::Prompt
# install File::Which
# install Perl::Unsafe::Signals
# install Bit::Vector
# install Inline::C
# install PerlIO::gzip

# Installing Bioperl with CPAN
# perl -MCPAN -e shell
# install Bundle::CPAN
# install Module::Build
# install Bundle::BioPerl

# Installing NCBI-BLAST (Wublast alternative if we have existing copy (no longer freely available))
# Present in server
```

# functional annotation (possible tool: Blast2GO)
```{r}

```

### figure for length distribution of novel assembly
```{r}
Ae_ref <- read.table("~/Desktop/Brassica_project/KIAT_RNA_seq/assembly/Ae_ref_plain.fa.length", header = F)

Ol_ref <- read.table("~/Desktop/Brassica_project/KIAT_RNA_seq/assembly/Ol_ref_plain.fa.length", header = F)

Ae_denovo <- read.table("~/Desktop/Brassica_project/KIAT_RNA_seq/assembly/Ae_denovo_plain.fa.length", header = F)

Ol_denovo <- read.table("~/Desktop/Brassica_project/KIAT_RNA_seq/assembly/Ol_denovo_plain.fa.length", header = F)

ref_CDS <- read.table("~/Desktop/Brassica_project/KIAT_RNA_seq/assembly/Brassica_napus_length.txt", header = F)

length.distr<- rbind(length.distr.calc(Ae_ref), length.distr.calc(Ol_ref), length.distr.calc(Ae_denovo), length.distr.calc(Ol_denovo), length.distr.calc(ref_CDS))

length.distr$range <- factor(length.distr$range, levels = c("<200","200-500","500-1000","1000-1500","1500-2000","2000-5000",">5000"))
  
# plot 
library(ggplot2)
p.length.distr <- ggplot(data = length.distr)
p.length.distr <- p.length.distr + geom_bar(aes(x=as.factor(range), y=percentage, fill=class), stat = "identity")
p.length.distr <- p.length.distr + facet_wrap(~class) 
p.length.distr <- p.length.distr + labs(list(title = "", x = "length range", y = "percentage"))
p.length.distr <- p.length.distr + theme(axis.text.x = element_text(angle = 90, size = 8))
p.length.distr 
```

### appendix function 
```{r}
length.distr.calc <- function(length.data){
  length.distr <- data.frame(range = c("<200","200-500","500-1000","1000-1500","1500-2000","2000-5000",">5000"), 
                             percentage = c(round(sum(length.data$V2<200)/nrow(length.data), digits = 2), round(sum(length.data$V2>=200 & length.data$V2<500)/nrow(length.data), digits = 2), round(sum(length.data$V2>=500 & length.data$V2<1000)/nrow(length.data), digits = 2),  round(sum(length.data$V2>=1000 & length.data$V2<1500)/nrow(length.data), digits = 2), round(sum(length.data$V2>=1500 & length.data$V2<2000)/nrow(length.data), digits = 2), round(sum(length.data$V2>=2000 & length.data$V2<5000)/nrow(length.data), digits = 2), round(sum(length.data$V2>=5000)/nrow(length.data), digits = 2)),
                          class = deparse(substitute(length.data)))
return(length.distr)
}
```

# Go enrichment for Arabidopsis 
```{r}
# ORA with GOseq
# prerequisit
library(ShortRead);library(goseq);library(GO.db);library("org.At.tair.db");library("annotate")

#TIR10_cdna_rep_model<-readDNAStringSet("../../Nozue2016_SAStranscriptome_data/input/TAIR10_cdna_20110103_representative_gene_model") 
#setwd("../")
TIR10_cdna_rep_model<-readDNAStringSet(file.path(homedir,"..","..","Nozue2016_SAStranscriptome_data/input/TAIR10_cdna_20110103_representative_gene_model") ) ### update this reference gene list 
#setwd(homedir2)
head(TIR10_cdna_rep_model)
bias<-nchar(TIR10_cdna_rep_model)
names(bias)<-substr(names(TIR10_cdna_rep_model),1,9)
length(bias) 
#  bias.data vector must have the same length as DEgenes vector!
###Read in AtGO
Atgo <- toTable(org.At.tairGO)
#head(Atgo)
BP <- TRUE #only keep BP go TERMS
if (BP) Atgo <- Atgo[Atgo$Ontology=="BP",]
#convert to list
Atgo.list <- tapply(Atgo$go_id,Atgo$gene_id,c)

GOseq.ORA<-function(genelist,padjust=0.05) { # return GO enrichment table, padjus, padjust=0.05 , modified 092817
  TF<-(names(bias) %in% genelist)*1
  names(TF)<-names(bias)
  #print(TF)
  pwf<-nullp(TF,bias.data=bias)
  #print(pwf$DEgenes)
  ###Read in AtGO
  Atgo <- toTable(org.At.tairGO)
  #head(Atgo)
  BP <- TRUE #only keep BP go TERMS
  if (BP) Atgo <- Atgo[Atgo$Ontology=="BP",]
  #convert to list
  Atgo.list <- tapply(Atgo$go_id,Atgo$gene_id,c)
  #
  GO.pval <- goseq(pwf,gene2cat=Atgo.list,use_genes_without_cat=TRUE) # format became different in new goseq version (021111)
  #head(GO.pval) 
  GO.pval$over_represented_padjust<-p.adjust(GO.pval$over_represented_pvalue,method="BH")
  #if(GO.pval$over_represented_padjust[1]>padjust) stop("no enriched GO")
  if(GO.pval$over_represented_padjust[1]>padjust) return("no enriched GO")
  
  else {
    enriched.GO<-GO.pval[GO.pval$over_represented_padjust<padjust,] 
    print("enriched.GO is")
    print(enriched.GO)
    
    ## write Term and Definition 
    for(i in 1:dim(enriched.GO)[1]) {
      enriched.GO$Term[i]<-Term(GOTERM[[enriched.GO[i,"category"]]])
      enriched.GO$Definition[i]<-Definition(GOTERM[[enriched.GO[i,"category"]]])
    }
    return(enriched.GO)
  }
}
# example
#enriched.GO.Col.SOMcluster1<-GOseq.ORA(rownames(data.val3.4.SOM.Col.SOM1.all.barcode.s)[rownames(data.val3.4.SOM.Col.SOM1.all.barcode.s) %in% names(bias)]) 
```

